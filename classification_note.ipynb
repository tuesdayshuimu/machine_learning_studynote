{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b658cd9-6a5d-4c99-bdb4-3bb660900214",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "### Use cases\n",
    "Email filtering, \n",
    "speech recognition,\n",
    "handwriting recognition\n",
    "biometric identification\n",
    "document classification\n",
    "\n",
    "### Types\n",
    "binary classification\n",
    "multiple class classification\n",
    "\n",
    "### Algorithms\n",
    "Decision trees (ID3, C4.5, C5.0)\n",
    "Naive bayes\n",
    "Linear discriminant analysis\n",
    "k-Nearest neighbor\n",
    "Logistic regression\n",
    "Neural network\n",
    "Support vector machines (SVM)\n",
    "\n",
    "### KNN\n",
    "KNN is, \n",
    "- a method for classifying cases based on their similarity to other cases.\n",
    "- cases that are near each other are said to be \"neighbors\"\n",
    "- based on similar case with same class labels are near each other\n",
    "\n",
    "Algorithm is (steps):\n",
    "1. pick a value for K\n",
    "2. calculate the distance of unknown case from all cases\n",
    "3. select the k-observations in the training data that are \"nearest\" to the unknown data point\n",
    "4. predict the response of the unknown data point using the most popular response value from the k-nearest neighbors\n",
    "\n",
    "Evaluation Metrics\n",
    "- Jaccard index (simpliest), J(y,y_hat) = |y ^ y_hat| / |y v y_hat|, 1 is the best, 0 is the worst, confusion matrix,\n",
    "- F1-score\n",
    "  - precision, tp/(tp+fp)\n",
    "  - recall, tp/(tp+fn)\n",
    "  - score equation, 2*(prc * rec)/(prc+rec)\n",
    "  - 1 is the best, 0 is the best\n",
    "  - can be used in multiclass classifiers as well\n",
    "- Log loss (logarithmic loss)\n",
    "  - equation, -(1/n)*sum((y*log(y_hat)+(1-y)*log(1-y_hat)))\n",
    "  - 0 is the best, 1 is the worst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950ad0cf-a80e-4c09-9ae8-588d91156790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the library\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# training\n",
    "k = 4\n",
    "neigh = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)\n",
    "print(neigh)\n",
    "\n",
    "# predicting\n",
    "yhat = neigh.predict(X_test)\n",
    "print(yhat[0:5])\n",
    "\n",
    "# accuracy evaluation\n",
    "from sklearn import metrics\n",
    "# In multilabel classification, accuracy classification score is a function that computes subset accuracy. \n",
    "# This function is equal to the jaccard_score function.\n",
    "print(\"Train set Accuracy: \", metrics.accuracy_score(y_train, neigh.predict(X_train)))\n",
    "print(\"Test set Accuracy: \", metrics.accuracy_score(y_test, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd7d220-d3e7-448d-b40f-694a5fc447ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "how can we choose right value for K? \n",
    "The general solution is to reserve a part of your data for testing the accuracy of the model. \n",
    "Then choose k =1, use the training part for modeling, and calculate the accuracy of prediction using all samples in your test set. \n",
    "Repeat this process, increasing the k, and see which k is the best for your model.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7998fd-dc46-4007-81d7-e4b69fe57c25",
   "metadata": {},
   "source": [
    "### Standardization (z-score normalization)\n",
    "##### Formula: z = (x - mu) / sigma\n",
    "##### Effect: \n",
    "- Transforms the feature to have a mean of 0 and a standard deviation of 1.\n",
    "- Helps handle features with different units or scales.\n",
    "##### When to use\n",
    "- When the data is normally distributed or when features have significantly different variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c530435c-fd50-4f2d-a4a4-bce2728f45d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682935eb-7b46-4476-a8df-e1548afb1dc7",
   "metadata": {},
   "source": [
    "### Min-Max Scaling (Normalization)\n",
    "\n",
    "##### Formula\n",
    "x' = [x - min(x)] / [max(x) - min(x)]\n",
    "\n",
    "##### Effect\n",
    "rescales all features to fall within the same range, making comparisons straightforward.\n",
    "\n",
    "##### When to use\n",
    "When you want all features to have a comparable scale without altering their original distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a883763-036d-40d9-892d-0948cf4a0123",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6490eb5-05ed-42e1-b8b3-6f42a786c021",
   "metadata": {},
   "source": [
    "### Robus Scaling\n",
    "\n",
    "##### Formula:\n",
    "x' = [x = median(x)] / IQR\n",
    "\n",
    "##### Effect\n",
    "Scales data based on median and interquartile range, making it robust to outliers\n",
    "\n",
    "##### When to use\n",
    "When the dataset has many outliers that could skew standardization or normalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d83123-a571-4eb4-bb08-6d356d7711c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b9fcc0-2722-42cc-be19-773942b909cc",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "decision trees are built by splitting the training set into distinct nodes,\n",
    "where one node contains all of or most of one category of the data.\n",
    "\n",
    "decision trees are about testing an attribute and branching the cases based on the result of the test.\n",
    "each internal nodes corresponds to a test\n",
    "each branch corresponds to a result of the test\n",
    "each leaf node assigns a patient to a class\n",
    "\n",
    "contructed by considering the attributes one by one, the algorithm is:\n",
    "1. choose an attributes from out dataset\n",
    "2. calculate the significance of the attribute in the splitting of the data\n",
    "3. split the data based on the value of the best attribute\n",
    "4. go to each branch and repeat it for the rest of the attributes\n",
    "\n",
    "decision trees are built using recursive partitioning to classify the data\n",
    "\n",
    "more predictiveness\n",
    "less impurity\n",
    "lower entropy\n",
    "\n",
    "entropy is the amount of information disorder or the amount of randomness in the data\n",
    "used to calculate the homogeneity of the samples in that node.\n",
    "the lower the entropy, the less uniform the distribution, the purer the node \n",
    "entropy formula: -p(A)log2(p(A)) - p(B)log2(p(B))\n",
    "\n",
    "The tree with the higher information gain after splitting.\n",
    "is the information that can increase the level of certainty after splitting.\n",
    "information gain = (Entropy before split) - (weighted entropy after split)\n",
    "\n",
    "higher information gain and lower entropy\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
