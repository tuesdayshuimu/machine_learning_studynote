{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b658cd9-6a5d-4c99-bdb4-3bb660900214",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "### Use cases\n",
    "Email filtering, \n",
    "speech recognition,\n",
    "handwriting recognition\n",
    "biometric identification\n",
    "document classification\n",
    "\n",
    "### Types\n",
    "binary classification\n",
    "multiple class classification\n",
    "\n",
    "### Algorithms\n",
    "Decision trees (ID3, C4.5, C5.0)\n",
    "Naive bayes\n",
    "Linear discriminant analysis\n",
    "k-Nearest neighbor\n",
    "Logistic regression\n",
    "Neural network\n",
    "Support vector machines (SVM)\n",
    "\n",
    "### KNN\n",
    "KNN is, \n",
    "- a method for classifying cases based on their similarity to other cases.\n",
    "- cases that are near each other are said to be \"neighbors\"\n",
    "- based on similar case with same class labels are near each other\n",
    "\n",
    "Algorithm is (steps):\n",
    "1. pick a value for K\n",
    "2. calculate the distance of unknown case from all cases\n",
    "3. select the k-observations in the training data that are \"nearest\" to the unknown data point\n",
    "4. predict the response of the unknown data point using the most popular response value from the k-nearest neighbors\n",
    "\n",
    "Evaluation Metrics\n",
    "- Jaccard index (simpliest), J(y,y_hat) = |y ^ y_hat| / |y v y_hat|, 1 is the best, 0 is the worst, confusion matrix,\n",
    "- F1-score\n",
    "  - precision, tp/(tp+fp)\n",
    "  - recall, tp/(tp+fn)\n",
    "  - score equation, 2*(prc * rec)/(prc+rec)\n",
    "  - 1 is the best, 0 is the best\n",
    "  - can be used in multiclass classifiers as well\n",
    "- Log loss (logarithmic loss)\n",
    "  - equation, -(1/n)*sum((y*log(y_hat)+(1-y)*log(1-y_hat)))\n",
    "  - 0 is the best, 1 is the worst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950ad0cf-a80e-4c09-9ae8-588d91156790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the library\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# training\n",
    "k = 4\n",
    "neigh = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)\n",
    "print(neigh)\n",
    "\n",
    "# predicting\n",
    "yhat = neigh.predict(X_test)\n",
    "print(yhat[0:5])\n",
    "\n",
    "# accuracy evaluation\n",
    "from sklearn import metrics\n",
    "# In multilabel classification, accuracy classification score is a function that computes subset accuracy. \n",
    "# This function is equal to the jaccard_score function.\n",
    "print(\"Train set Accuracy: \", metrics.accuracy_score(y_train, neigh.predict(X_train)))\n",
    "print(\"Test set Accuracy: \", metrics.accuracy_score(y_test, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd7d220-d3e7-448d-b40f-694a5fc447ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "how can we choose right value for K? \n",
    "The general solution is to reserve a part of your data for testing the accuracy of the model. \n",
    "Then choose k =1, use the training part for modeling, and calculate the accuracy of prediction using all samples in your test set. \n",
    "Repeat this process, increasing the k, and see which k is the best for your model.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24f1612-c534-4124-aa4a-8a08f6c24ab0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
